
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../VAE.layers/">
      
      
        <link rel="next" href="../VAE.callbacks/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.5.21">
    
    
      
        <title>Losses - VAE project</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.66ac8b77.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#VAE.losses" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="VAE project" class="md-header__button md-logo" aria-label="VAE project" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            VAE project
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Losses
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/andr-groth/VAE-project" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="VAE project" class="md-nav__button md-logo" aria-label="VAE project" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    VAE project
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/andr-groth/VAE-project" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Overview
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Build model
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Build model
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../VAE.models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../VAE.layers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Layers
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Train model
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Train model
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Losses
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Losses
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#VAE.losses" class="md-nav__link">
    <span class="md-ellipsis">
      losses
    </span>
  </a>
  
    <nav class="md-nav" aria-label="losses">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#VAE.losses.KLDivergence" class="md-nav__link">
    <span class="md-ellipsis">
      KLDivergence
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#VAE.losses.SquaredError" class="md-nav__link">
    <span class="md-ellipsis">
      SquaredError
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#VAE.losses.Similarity" class="md-nav__link">
    <span class="md-ellipsis">
      Similarity
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#VAE.losses.SimilarityBetween" class="md-nav__link">
    <span class="md-ellipsis">
      SimilarityBetween
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#VAE.losses.TotalCorrelation" class="md-nav__link">
    <span class="md-ellipsis">
      TotalCorrelation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#VAE.losses.TotalCorrelationBetween" class="md-nav__link">
    <span class="md-ellipsis">
      TotalCorrelationBetween
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#VAE.losses.TotalCorrelationWithin" class="md-nav__link">
    <span class="md-ellipsis">
      TotalCorrelationWithin
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#VAE.losses.VAEloss" class="md-nav__link">
    <span class="md-ellipsis">
      VAEloss
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#VAE.losses.VAEploss" class="md-nav__link">
    <span class="md-ellipsis">
      VAEploss
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#VAE.losses.example_total_correlation_losses" class="md-nav__link">
    <span class="md-ellipsis">
      example_total_correlation_losses
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#VAE.losses.example_similarity_losses" class="md-nav__link">
    <span class="md-ellipsis">
      example_similarity_losses
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../VAE.callbacks/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Callbacks
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../VAE.logs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Logs
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../VAE.generators/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Generators
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../VAE.utils.beta_schedulers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Beta Scheduler
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Utils
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Utils
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../VAE.utils.fileio/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    FileIO
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../VAE.utils.math/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Mathematical functions
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../VAE.utils.collection/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Helper functions
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../VAE.utils.plot/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Plot functions
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Examples
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Examples
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../example_VAE/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    VAE
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../example_VAEp/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    VAEp
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ... more
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#VAE.losses" class="md-nav__link">
    <span class="md-ellipsis">
      losses
    </span>
  </a>
  
    <nav class="md-nav" aria-label="losses">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#VAE.losses.KLDivergence" class="md-nav__link">
    <span class="md-ellipsis">
      KLDivergence
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#VAE.losses.SquaredError" class="md-nav__link">
    <span class="md-ellipsis">
      SquaredError
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#VAE.losses.Similarity" class="md-nav__link">
    <span class="md-ellipsis">
      Similarity
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#VAE.losses.SimilarityBetween" class="md-nav__link">
    <span class="md-ellipsis">
      SimilarityBetween
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#VAE.losses.TotalCorrelation" class="md-nav__link">
    <span class="md-ellipsis">
      TotalCorrelation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#VAE.losses.TotalCorrelationBetween" class="md-nav__link">
    <span class="md-ellipsis">
      TotalCorrelationBetween
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#VAE.losses.TotalCorrelationWithin" class="md-nav__link">
    <span class="md-ellipsis">
      TotalCorrelationWithin
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#VAE.losses.VAEloss" class="md-nav__link">
    <span class="md-ellipsis">
      VAEloss
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#VAE.losses.VAEploss" class="md-nav__link">
    <span class="md-ellipsis">
      VAEploss
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#VAE.losses.example_total_correlation_losses" class="md-nav__link">
    <span class="md-ellipsis">
      example_total_correlation_losses
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#VAE.losses.example_similarity_losses" class="md-nav__link">
    <span class="md-ellipsis">
      example_similarity_losses
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  <h1>Losses</h1>

<div class="doc doc-object doc-module">



<h2 id="VAE.losses" class="doc doc-heading">
          <span class="doc doc-object-name doc-module-name">VAE.losses</span>


</h2>

  <div class="doc doc-contents first">
  
      <p>Collection of loss functions.</p>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h3 id="VAE.losses.KLDivergence" class="doc doc-heading">
          <span class="doc doc-object-name doc-function-name">VAE.losses.KLDivergence</span>


</h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">KLDivergence</span><span class="p">(</span><span class="n">z_mean</span><span class="p">,</span> <span class="n">z_log_var</span><span class="p">,</span> <span class="n">kl_threshold</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">free_bits</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Kullback-Leibler divergence.</p>
<p>This is the KL divergence between N(<code>z_mean</code>, <code>z_log_var</code>) and the prior N(0, 1).</p>



  <p><span class="doc-section-title">Parameters:</span></p>
  <ul>
      <li class="doc-section-item field-body">
        <b><code>z_mean</code></b>
            (<code><span title="tensorflow.Tensor">Tensor</span></code>)
        –
        <div class="doc-md-description">
          <p>Tensor of shape <code>(batch_size, latent_dim)</code> specifying mean.</p>
        </div>
      </li>
      <li class="doc-section-item field-body">
        <b><code>z_log_var</code></b>
            (<code><span title="tensorflow.Tensor">Tensor</span></code>)
        –
        <div class="doc-md-description">
          <p>Tensor of shape <code>(batch_size, latent_dim)</code> specifying log of variance.</p>
        </div>
      </li>
      <li class="doc-section-item field-body">
        <b><code>kl_threshold</code></b>
            (<code>float</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Lower bound for the KL divergence. Default is <code>None</code>.</p>
        </div>
      </li>
      <li class="doc-section-item field-body">
        <b><code>free_bits</code></b>
            (<code>float</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Number of bits to keep free for the KL divergence per latent dimension; cf. Appendix C8 in [1]. Default is
<code>None</code>.</p>
        </div>
      </li>
  </ul>



  <p><span class="doc-section-title">Returns:</span></p>
  <ul>
      <li class="doc-section-item field-body">
            <code>callable</code>
        –
        <div class="doc-md-description">
          <p>Loss function that returns a tensor of shape <code>(batch_size, 1, 1)</code>.</p>
      </div>
      </li>
  </ul>

<details class="references" open>
  <summary>References</summary>
  <p>[1] Kingma et al. (2016): Improved Variational Inference with Inverse Autoregressive Flow. NIPS 2016.</p>
</details>
          <details class="quote">
            <summary>Source code in <code>VAE/losses.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">KLDivergence</span><span class="p">(</span><span class="n">z_mean</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                 <span class="n">z_log_var</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                 <span class="n">kl_threshold</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">free_bits</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">callable</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Kullback-Leibler divergence.</span>

<span class="sd">    This is the KL divergence between N(`z_mean`, `z_log_var`) and the prior N(0, 1).</span>

<span class="sd">    Parameters:</span>
<span class="sd">        z_mean:</span>
<span class="sd">            Tensor of shape `(batch_size, latent_dim)` specifying mean.</span>
<span class="sd">        z_log_var:</span>
<span class="sd">            Tensor of shape `(batch_size, latent_dim)` specifying log of variance.</span>
<span class="sd">        kl_threshold:</span>
<span class="sd">            Lower bound for the KL divergence. Default is `None`.</span>
<span class="sd">        free_bits:</span>
<span class="sd">            Number of bits to keep free for the KL divergence per latent dimension; cf. Appendix C8 in [1]. Default is</span>
<span class="sd">            `None`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Loss function that returns a tensor of shape `(batch_size, 1, 1)`.</span>

<span class="sd">    References:</span>
<span class="sd">        [1] Kingma et al. (2016): Improved Variational Inference with Inverse Autoregressive Flow. NIPS 2016.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">kl_divergence</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
        <span class="c1"># KL divergence to N(0, 1) of shape (batch_size, latent_dim)</span>
        <span class="n">kl_loss</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">z_log_var</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">z_mean</span><span class="p">)</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z_log_var</span><span class="p">)</span>
        <span class="n">kl_loss</span> <span class="o">*=</span> <span class="o">-</span><span class="mf">0.5</span>

        <span class="c1"># apply threshold per latent dimension</span>
        <span class="k">if</span> <span class="n">free_bits</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">kl_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">kl_loss</span><span class="p">,</span> <span class="n">free_bits</span><span class="p">)</span>

        <span class="c1"># reduce to shape (batch_size, 1)</span>
        <span class="n">kl_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">kl_loss</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># apply global threshold</span>
        <span class="k">if</span> <span class="n">kl_threshold</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">kl_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">kl_loss</span><span class="p">,</span> <span class="n">kl_threshold</span><span class="p">)</span>

        <span class="c1"># expand to shape (batch_size, 1, 1)</span>
        <span class="n">kl_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">kl_loss</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">kl_loss</span>

    <span class="k">return</span> <span class="n">kl_divergence</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="VAE.losses.SquaredError" class="doc doc-heading">
          <span class="doc doc-object-name doc-function-name">VAE.losses.SquaredError</span>


</h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">SquaredError</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">taper</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Squared error loss.</p>
<p>This is the reconstruction loss of the model, without the KL divergence.</p>



  <p><span class="doc-section-title">Parameters:</span></p>
  <ul>
      <li class="doc-section-item field-body">
        <b><code>size</code></b>
            (<code>int</code>, default:
                <code>1</code>
)
        –
        <div class="doc-md-description">
          <p>Size of the model output of shape <code>(set_size, output_length, channels)</code>.</p>
        </div>
      </li>
      <li class="doc-section-item field-body">
        <b><code>taper</code></b>
            (<code>array_like</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Array of length <code>output_length</code> to taper the squared error.</p>
        </div>
      </li>
  </ul>



  <p><span class="doc-section-title">Returns:</span></p>
  <ul>
      <li class="doc-section-item field-body">
            <code>callable</code>
        –
        <div class="doc-md-description">
          <p>Loss function that returns a tensor of shape <code>(batch_size, set_size, 1)</code>.</p>
      </div>
      </li>
  </ul>

          <details class="quote">
            <summary>Source code in <code>VAE/losses.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span>
<span class="normal">89</span>
<span class="normal">90</span>
<span class="normal">91</span>
<span class="normal">92</span>
<span class="normal">93</span>
<span class="normal">94</span>
<span class="normal">95</span>
<span class="normal">96</span>
<span class="normal">97</span>
<span class="normal">98</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">SquaredError</span><span class="p">(</span><span class="n">size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">taper</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">callable</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Squared error loss.</span>

<span class="sd">    This is the reconstruction loss of the model, without the KL divergence.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        size:</span>
<span class="sd">            Size of the model output of shape `(set_size, output_length, channels)`.</span>
<span class="sd">        taper (array_like):</span>
<span class="sd">            Array of length `output_length` to taper the squared error.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Loss function that returns a tensor of shape `(batch_size, set_size, 1)`.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">squared_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
        <span class="c1"># losses reduce last channel dimension to shape (batch_size, set_size, output_length)</span>
        <span class="n">reconstruction_loss</span> <span class="o">=</span> <span class="n">ks</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">mse</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">taper</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">reconstruction_loss</span> <span class="o">*=</span> <span class="n">taper</span>
        <span class="c1"># further reduce to shape (batch_size, set_size, 1)</span>
        <span class="n">reconstruction_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">reconstruction_loss</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="c1"># scale back to sum of squared errors</span>
        <span class="n">reconstruction_loss</span> <span class="o">*=</span> <span class="n">size</span>

        <span class="k">return</span> <span class="n">reconstruction_loss</span>

    <span class="k">return</span> <span class="n">squared_error</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="VAE.losses.Similarity" class="doc doc-heading">
          <span class="doc doc-object-name doc-function-name">VAE.losses.Similarity</span>


</h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">Similarity</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Similarity loss.</p>
<p>This loss flattens all but the leading batch dimension of <code>y_pred</code> to the shape <code>(batch_size, -1)</code>. The similarity
is then calculated of the reshaped input.</p>



  <p><span class="doc-section-title">Parameters:</span></p>
  <ul>
      <li class="doc-section-item field-body">
        <b><code>temperature</code></b>
            (<code>float</code>, default:
                <code>1.0</code>
)
        –
        <div class="doc-md-description">
          <p>Temperature for the softmax.</p>
        </div>
      </li>
  </ul>
      <p>Returns:
    Loss function.</p>

          <details class="quote">
            <summary>Source code in <code>VAE/losses.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">Similarity</span><span class="p">(</span><span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">callable</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Similarity loss.</span>

<span class="sd">    This loss flattens all but the leading batch dimension of `y_pred` to the shape `(batch_size, -1)`. The similarity</span>
<span class="sd">    is then calculated of the reshaped input.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        temperature:</span>
<span class="sd">            Temperature for the softmax.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Loss function.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">sim</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c1"># flatten input to shape (batch_size, -1)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="c1"># compute similarity loss</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">vaemath</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">)</span>
        <span class="c1"># broadcast loss to shape (batch_size, 1, 1)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">return</span> <span class="n">sim</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="VAE.losses.SimilarityBetween" class="doc doc-heading">
          <span class="doc doc-object-name doc-function-name">VAE.losses.SimilarityBetween</span>


</h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">SimilarityBetween</span><span class="p">(</span><span class="n">repeat_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Similarity between repeated samples (fast implementation).</p>
<p>This function returns the similarity between repeated samples. The input <code>y_pred</code> is first reshaped to the shape
<code>(batch_size // repeat_samples, repeat_samples, ...)</code> and the similarity is calculated for each slice along the
first dimension.</p>
<p>Note: This is an implementation with einsum that avoids calling :func:<code>math.similarity</code> with :func:<code>tf.map_fn</code>.</p>



  <p><span class="doc-section-title">Parameters:</span></p>
  <ul>
      <li class="doc-section-item field-body">
        <b><code>repeat_samples</code></b>
            (<code>int</code>, default:
                <code>1</code>
)
        –
        <div class="doc-md-description">
          <p>Number of repeated samples.</p>
        </div>
      </li>
      <li class="doc-section-item field-body">
        <b><code>temperature</code></b>
            (<code>float</code>, default:
                <code>1.0</code>
)
        –
        <div class="doc-md-description">
          <p>Temperature for the softmax.</p>
        </div>
      </li>
  </ul>
      <p>Returns:
    Loss function.</p>

          <details class="quote">
            <summary>Source code in <code>VAE/losses.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">SimilarityBetween</span><span class="p">(</span><span class="n">repeat_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">callable</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Similarity between repeated samples (fast implementation).</span>

<span class="sd">    This function returns the similarity between repeated samples. The input `y_pred` is first reshaped to the shape</span>
<span class="sd">    `(batch_size // repeat_samples, repeat_samples, ...)` and the similarity is calculated for each slice along the</span>
<span class="sd">    first dimension.</span>

<span class="sd">    Note: This is an implementation with einsum that avoids calling :func:`math.similarity` with :func:`tf.map_fn`.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        repeat_samples:</span>
<span class="sd">            Number of repeated samples.</span>
<span class="sd">        temperature:</span>
<span class="sd">            Temperature for the softmax.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Loss function.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">sim_between</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c1"># reshape to (batch_size // repreat_samples, repeat_samples, -1)</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span> <span class="o">//</span> <span class="n">repeat_samples</span><span class="p">,</span> <span class="n">repeat_samples</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="c1"># normalize input</span>
        <span class="n">l2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">l2_normalize</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># correlation matrices of slices along first axis, each of shape (repeat_samples, -1)</span>
        <span class="c1"># shape (batch_size // repreat_samples, repeat_samples, repeat_samples)</span>
        <span class="n">similarity</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;ijk, ilk -&gt; ijl&#39;</span><span class="p">,</span> <span class="n">l2</span><span class="p">,</span> <span class="n">l2</span><span class="p">)</span>
        <span class="c1"># reshape to (batch_size, repeat_samples)</span>
        <span class="n">similarity</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">similarity</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">repeat_samples</span><span class="p">))</span>
        <span class="c1"># apply temperature</span>
        <span class="n">similarity</span> <span class="o">/=</span> <span class="n">temperature</span>
        <span class="c1"># target labels = diagonal elements</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">repeat_samples</span><span class="p">),</span> <span class="p">(</span><span class="n">batch_size</span> <span class="o">//</span> <span class="n">repeat_samples</span><span class="p">,</span> <span class="p">))</span>
        <span class="c1"># cross entropy loss between target labels and similarity matrices</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">ks</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">sparse_categorical_crossentropy</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">similarity</span><span class="p">,</span> <span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># broadcast to shape (batch_size, 1, 1)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">return</span> <span class="n">sim_between</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="VAE.losses.TotalCorrelation" class="doc doc-heading">
          <span class="doc doc-object-name doc-function-name">VAE.losses.TotalCorrelation</span>


</h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">TotalCorrelation</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">z_mean</span><span class="p">,</span> <span class="n">z_log_var</span><span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Total correlation.</p>
<p>The total correlation is already part of the KL divergence, see KL decomposition in [1]. This function only returns
the batch-wise sampled total correlation loss that will be added on top of the KL divergence. The total correlation
is computed separately for each step along the axis of length <code>set_size</code> of the input tensor <code>z</code>.</p>
<p>Parameters:
    z:
        Sample from latent space of shape <code>(batch_size, set_size, latent_dim)</code>.
    z_mean:
        Mean of latent space of shape <code>(batch_size, latent_dim)</code>.
    z_log_var:
        Log of variance of latent space of shape <code>(batch_size, latent_dim)</code>.</p>



  <p><span class="doc-section-title">Returns:</span></p>
  <ul>
      <li class="doc-section-item field-body">
            <code>callable</code>
        –
        <div class="doc-md-description">
          <p>Loss function that returns a tensor of shape <code>(batch_size, set_size, 1)</code>.</p>
      </div>
      </li>
  </ul>

<details class="references" open>
  <summary>References</summary>
  <p>[1] Chen et al. (2018): Isolating sources of disentanglement in Variational Autoencoders.</p>
</details>
          <details class="quote">
            <summary>Source code in <code>VAE/losses.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">TotalCorrelation</span><span class="p">(</span><span class="n">z</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">z_mean</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">z_log_var</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">callable</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Total correlation.</span>

<span class="sd">    The total correlation is already part of the KL divergence, see KL decomposition in [1]. This function only returns</span>
<span class="sd">    the batch-wise sampled total correlation loss that will be added on top of the KL divergence. The total correlation</span>
<span class="sd">    is computed separately for each step along the axis of length `set_size` of the input tensor `z`.</span>

<span class="sd">     Parameters:</span>
<span class="sd">        z:</span>
<span class="sd">            Sample from latent space of shape `(batch_size, set_size, latent_dim)`.</span>
<span class="sd">        z_mean:</span>
<span class="sd">            Mean of latent space of shape `(batch_size, latent_dim)`.</span>
<span class="sd">        z_log_var:</span>
<span class="sd">            Log of variance of latent space of shape `(batch_size, latent_dim)`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Loss function that returns a tensor of shape `(batch_size, set_size, 1)`.</span>

<span class="sd">    References:</span>
<span class="sd">        [1] Chen et al. (2018): Isolating sources of disentanglement in Variational Autoencoders.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># expand to shape (batch_size, 1, latent_dim) for broadcasting with z of shape (batch_size, set_size, latent_dim)</span>
    <span class="n">z_mean</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">z_mean</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">z_log_var</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">z_log_var</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">tc</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="c1"># log prob of all combinations along first axis of length batch_size</span>
        <span class="c1"># shape (batch_size, batch_size, set_size, latent_dim)</span>
        <span class="n">mat_log_qz</span> <span class="o">=</span> <span class="n">vaemath</span><span class="o">.</span><span class="n">log_density_gaussian</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">z_mean</span><span class="p">,</span> <span class="n">z_log_var</span><span class="p">,</span> <span class="n">all_combinations</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># log prob of joint distribution of shape (batch_size, set_size, 1)</span>
        <span class="n">log_qz</span> <span class="o">=</span> <span class="n">vaemath</span><span class="o">.</span><span class="n">reduce_logmeanexp</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">mat_log_qz</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># log prob of product of marginal distributions of shape (batch_size, set_size, 1)</span>
        <span class="n">log_prod_qz</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">vaemath</span><span class="o">.</span><span class="n">reduce_logmeanexp</span><span class="p">(</span><span class="n">mat_log_qz</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># total correlation loss of shape (batch_size, set_size, 1)</span>
        <span class="n">tc_loss</span> <span class="o">=</span> <span class="n">log_qz</span> <span class="o">-</span> <span class="n">log_prod_qz</span>

        <span class="k">return</span> <span class="n">tc_loss</span>

    <span class="k">return</span> <span class="n">tc</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="VAE.losses.TotalCorrelationBetween" class="doc doc-heading">
          <span class="doc doc-object-name doc-function-name">VAE.losses.TotalCorrelationBetween</span>


</h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">TotalCorrelationBetween</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">z_mean</span><span class="p">,</span> <span class="n">z_log_var</span><span class="p">,</span> <span class="n">repeat_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Total correlation between repeated samples.</p>
<p>Returns the total correlation loss between repeated samples. This is the same as the total correlation but
restricted to the same repeated samples. This means that <code>z_mean</code> and <code>z_log_var</code> are split into segments of length
<code>repeat_samples</code> along the first axis and the total correlation is computed within each segment.</p>
<p>This version of the total correlation is useful for the case where the model is trained with repeated input samples.
It helps increase the diversity of the latent distribution between repeated samples.</p>



  <p><span class="doc-section-title">Parameters:</span></p>
  <ul>
      <li class="doc-section-item field-body">
        <b><code>z</code></b>
            (<code><span title="tensorflow.Tensor">Tensor</span></code>)
        –
        <div class="doc-md-description">
          <p>Sample from latent space of shape <code>(batch_size, set_size, latent_dim)</code>.</p>
        </div>
      </li>
      <li class="doc-section-item field-body">
        <b><code>z_mean</code></b>
            (<code><span title="tensorflow.Tensor">Tensor</span></code>)
        –
        <div class="doc-md-description">
          <p>Mean of latent space of shape <code>(batch_size, latent_dim)</code>.</p>
        </div>
      </li>
      <li class="doc-section-item field-body">
        <b><code>z_log_var</code></b>
            (<code><span title="tensorflow.Tensor">Tensor</span></code>)
        –
        <div class="doc-md-description">
          <p>Log of variance of latent space of shape <code>(batch_size, latent_dim)</code>.</p>
        </div>
      </li>
      <li class="doc-section-item field-body">
        <b><code>repeat_samples</code></b>
            (<code>int</code>, default:
                <code>1</code>
)
        –
        <div class="doc-md-description">
          <p>Number of repeated samples.</p>
        </div>
      </li>
  </ul>



  <p><span class="doc-section-title">Returns:</span></p>
  <ul>
      <li class="doc-section-item field-body">
            <code>callable</code>
        –
        <div class="doc-md-description">
          <p>Loss function that returns a tensor of shape <code>(batch_size, set_size, 1)</code>.</p>
      </div>
      </li>
  </ul>

          <details class="quote">
            <summary>Source code in <code>VAE/losses.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">TotalCorrelationBetween</span><span class="p">(</span><span class="n">z</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">z_mean</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">z_log_var</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">repeat_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">callable</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Total correlation between repeated samples.</span>

<span class="sd">    Returns the total correlation loss between repeated samples. This is the same as the total correlation but</span>
<span class="sd">    restricted to the same repeated samples. This means that `z_mean` and `z_log_var` are split into segments of length</span>
<span class="sd">    `repeat_samples` along the first axis and the total correlation is computed within each segment.</span>

<span class="sd">    This version of the total correlation is useful for the case where the model is trained with repeated input samples.</span>
<span class="sd">    It helps increase the diversity of the latent distribution between repeated samples.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        z:</span>
<span class="sd">            Sample from latent space of shape `(batch_size, set_size, latent_dim)`.</span>
<span class="sd">        z_mean:</span>
<span class="sd">            Mean of latent space of shape `(batch_size, latent_dim)`.</span>
<span class="sd">        z_log_var:</span>
<span class="sd">            Log of variance of latent space of shape `(batch_size, latent_dim)`.</span>
<span class="sd">        repeat_samples:</span>
<span class="sd">            Number of repeated samples.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Loss function that returns a tensor of shape `(batch_size, set_size, 1)`.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># reshape z_mean and z_log_var to shape (batch_size // repeat_samples, repeat_samples, 1, latent_dim)</span>
    <span class="n">z_mean</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">z_mean</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">repeat_samples</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">z_mean</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
    <span class="n">z_log_var</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">z_log_var</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">repeat_samples</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">z_log_var</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>

    <span class="c1"># reshape z to shape (batch_size // repeat_samples, repeat_samples, set_size, latent_dim)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">repeat_samples</span><span class="p">,</span> <span class="n">z</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">z</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>

    <span class="k">def</span> <span class="nf">tc_between</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
        <span class="c1"># log prob of all combinations along second axis of size repeat_samples</span>
        <span class="c1"># shape (batch_size // repeat_samples, repeat_samples, repeat_samples, set_size, latent_dim)</span>
        <span class="n">mat_log_qz</span> <span class="o">=</span> <span class="n">vaemath</span><span class="o">.</span><span class="n">log_density_gaussian</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">z_mean</span><span class="p">,</span> <span class="n">z_log_var</span><span class="p">,</span> <span class="n">all_combinations</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># log prob of joint distribution, shape (batch_size // repeat_samples, repeat_samples, set_size, 1)</span>
        <span class="n">log_qz</span> <span class="o">=</span> <span class="n">vaemath</span><span class="o">.</span><span class="n">reduce_logmeanexp</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">mat_log_qz</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="c1"># log prob of product of marg. distribution, shape (batch_size // repeat_samples, repeat_samples, set_size, 1)</span>
        <span class="n">log_prod_qz</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">vaemath</span><span class="o">.</span><span class="n">reduce_logmeanexp</span><span class="p">(</span><span class="n">mat_log_qz</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># total correlation loss, shape (batch_size // repeat_samples, repeat_samples, set_size, 1)</span>
        <span class="n">tc_loss</span> <span class="o">=</span> <span class="n">log_qz</span> <span class="o">-</span> <span class="n">log_prod_qz</span>

        <span class="c1"># reshape to shape (batch_size, set_size, 1)</span>
        <span class="n">tc_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tc_loss</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">tc_loss</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">tc_loss</span>

    <span class="k">return</span> <span class="n">tc_between</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="VAE.losses.TotalCorrelationWithin" class="doc doc-heading">
          <span class="doc doc-object-name doc-function-name">VAE.losses.TotalCorrelationWithin</span>


</h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">TotalCorrelationWithin</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">z_mean</span><span class="p">,</span> <span class="n">z_log_var</span><span class="p">,</span> <span class="n">repeat_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Total correlation within repeated samples.</p>
<p>Returns the total correlation loss within all samples of same repetition. This is the same as the total correlation
but restricted to samples of the same repetition. This means that <code>z_mean</code> and <code>z_log_var</code> are split into strided
views with stride <code>repeat_samples</code> along the first axis and the total correlation is computed within each view.</p>



  <p><span class="doc-section-title">Parameters:</span></p>
  <ul>
      <li class="doc-section-item field-body">
        <b><code>z</code></b>
            (<code><span title="tensorflow.Tensor">Tensor</span></code>)
        –
        <div class="doc-md-description">
          <p>Sample from latent space of shape <code>(batch_size, set_size, latent_dim)</code>.</p>
        </div>
      </li>
      <li class="doc-section-item field-body">
        <b><code>z_mean</code></b>
            (<code><span title="tensorflow.Tensor">Tensor</span></code>)
        –
        <div class="doc-md-description">
          <p>Mean of latent space of shape <code>(batch_size, latent_dim)</code>.</p>
        </div>
      </li>
      <li class="doc-section-item field-body">
        <b><code>z_log_var</code></b>
            (<code><span title="tensorflow.Tensor">Tensor</span></code>)
        –
        <div class="doc-md-description">
          <p>Log of variance of latent space of shape <code>(batch_size, latent_dim)</code>.</p>
        </div>
      </li>
      <li class="doc-section-item field-body">
        <b><code>repeat_samples</code></b>
            (<code>int</code>, default:
                <code>1</code>
)
        –
        <div class="doc-md-description">
          <p>Number of repeated samples.</p>
        </div>
      </li>
  </ul>



  <p><span class="doc-section-title">Returns:</span></p>
  <ul>
      <li class="doc-section-item field-body">
            <code>callable</code>
        –
        <div class="doc-md-description">
          <p>Loss function that returns a tensor of shape <code>(batch_size, set_size, 1)</code>.</p>
      </div>
      </li>
  </ul>

          <details class="quote">
            <summary>Source code in <code>VAE/losses.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">TotalCorrelationWithin</span><span class="p">(</span><span class="n">z</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">z_mean</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">z_log_var</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">repeat_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">callable</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Total correlation within repeated samples.</span>

<span class="sd">    Returns the total correlation loss within all samples of same repetition. This is the same as the total correlation</span>
<span class="sd">    but restricted to samples of the same repetition. This means that `z_mean` and `z_log_var` are split into strided</span>
<span class="sd">    views with stride `repeat_samples` along the first axis and the total correlation is computed within each view.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        z:</span>
<span class="sd">            Sample from latent space of shape `(batch_size, set_size, latent_dim)`.</span>
<span class="sd">        z_mean:</span>
<span class="sd">            Mean of latent space of shape `(batch_size, latent_dim)`.</span>
<span class="sd">        z_log_var:</span>
<span class="sd">            Log of variance of latent space of shape `(batch_size, latent_dim)`.</span>
<span class="sd">        repeat_samples:</span>
<span class="sd">            Number of repeated samples.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Loss function that returns a tensor of shape `(batch_size, set_size, 1)`.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># reshape z_mean and z_log_var to shape (batch_size // repeat_samples, repeat_samples, 1, latent_dim)</span>
    <span class="n">z_mean</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">z_mean</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">repeat_samples</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">z_mean</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
    <span class="n">z_log_var</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">z_log_var</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">repeat_samples</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">z_log_var</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>

    <span class="c1"># reshape z to shape (batch_size // repeat_samples, repeat_samples, set_size, latent_dim)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">repeat_samples</span><span class="p">,</span> <span class="n">z</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">z</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>

    <span class="k">def</span> <span class="nf">tc_within</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
        <span class="c1"># log prob of all combinations along first axis of size batch_size // repeat_samples</span>
        <span class="c1"># shape (batch_size // repeat_samples, batch_size // repeat_samples, repeat_samples, set_size, latent_dim)</span>
        <span class="n">mat_log_qz</span> <span class="o">=</span> <span class="n">vaemath</span><span class="o">.</span><span class="n">log_density_gaussian</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">z_mean</span><span class="p">,</span> <span class="n">z_log_var</span><span class="p">,</span> <span class="n">all_combinations</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># log prob of joint distribution, shape (batch_size // repeat_samples, repeat_samples, set_size, 1)</span>
        <span class="n">log_qz</span> <span class="o">=</span> <span class="n">vaemath</span><span class="o">.</span><span class="n">reduce_logmeanexp</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">mat_log_qz</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># log prob of product of marg. distribution, shape (batch_size // repeat_samples, repeat_samples, set_size, 1)</span>
        <span class="n">log_prod_qz</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">vaemath</span><span class="o">.</span><span class="n">reduce_logmeanexp</span><span class="p">(</span><span class="n">mat_log_qz</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># total correlation loss, shape (batch_size // repeat_samples, repeat_samples, set_size, 1)</span>
        <span class="n">tc_loss</span> <span class="o">=</span> <span class="n">log_qz</span> <span class="o">-</span> <span class="n">log_prod_qz</span>

        <span class="c1"># revert shape to (batch_size, set_size, 1)</span>
        <span class="n">tc_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tc_loss</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">tc_loss</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">tc_loss</span>

    <span class="k">return</span> <span class="n">tc_within</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="VAE.losses.VAEloss" class="doc doc-heading">
          <span class="doc doc-object-name doc-function-name">VAE.losses.VAEloss</span>


</h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">VAEloss</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">z_mean</span><span class="p">,</span> <span class="n">z_log_var</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">gamma_between</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">gamma_within</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">delta</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">delta_between</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">kl_threshold</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">free_bits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">repeat_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">taper</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Variational auto-encoder loss function.</p>
<p>Loss function of variational auto-encoder, for use in :func:<code>models.VAE</code>. The input to the loss function has shape
<code>(batch_size, set_size, output_length, channels)</code>. The output of the loss function has shape <code>(batch_size, set_size,
1)</code>. The sample weights from the generator must have shape <code>(batch_size, set_size)</code>. This will make the sample
weights sample-dependent; see also <code>sample_weight_mode='temporal'</code> in model compile.</p>



  <p><span class="doc-section-title">Parameters:</span></p>
  <ul>
      <li class="doc-section-item field-body">
        <b><code>z</code></b>
            (<code><span title="tensorflow.Tensor">Tensor</span></code>)
        –
        <div class="doc-md-description">
          <p>Sample from latent space of shape <code>(batch_size, set_size, latent_dim)</code>.</p>
        </div>
      </li>
      <li class="doc-section-item field-body">
        <b><code>z_mean</code></b>
            (<code><span title="tensorflow.Tensor">Tensor</span></code>)
        –
        <div class="doc-md-description">
          <p>Mean of latent space of shape <code>(batch_size, latent_dim)</code>.</p>
        </div>
      </li>
      <li class="doc-section-item field-body">
        <b><code>z_log_var</code></b>
            (<code><span title="tensorflow.Tensor">Tensor</span></code>)
        –
        <div class="doc-md-description">
          <p>Log of variance of latent space of shape <code>(batch_size, latent_dim)</code>.</p>
        </div>
      </li>
      <li class="doc-section-item field-body">
        <b><code>size</code></b>
            (<code>int</code>, default:
                <code>1</code>
)
        –
        <div class="doc-md-description">
          <p>Size of decoder output, i.e. total number of elements.</p>
        </div>
      </li>
      <li class="doc-section-item field-body">
        <b><code>beta</code></b>
            (<code><span title="typing.Union">Union</span>[float, <span title="tensorflow.Tensor">Tensor</span>]</code>, default:
                <code>1.0</code>
)
        –
        <div class="doc-md-description">
          <p>Loss weight of the KL divergence. If <code>beta</code> is a float, the loss weight is constant. If <code>beta</code> is a
tensor, it should have shape <code>(batch_size, 1)</code>.</p>
        </div>
      </li>
      <li class="doc-section-item field-body">
        <b><code>gamma</code></b>
            (<code>float</code>, default:
                <code>0.0</code>
)
        –
        <div class="doc-md-description">
          <p>Scale of total correlation loss. See :func:<code>losses.TotalCorrelation</code>.</p>
        </div>
      </li>
      <li class="doc-section-item field-body">
        <b><code>gamma_between</code></b>
            (<code>float</code>, default:
                <code>0.0</code>
)
        –
        <div class="doc-md-description">
          <p>Scale of total correlation loss between repeated samples. See :func:<code>losses.TotalCorrelationBetween</code>.</p>
        </div>
      </li>
      <li class="doc-section-item field-body">
        <b><code>gamma_within</code></b>
            (<code>float</code>, default:
                <code>0.0</code>
)
        –
        <div class="doc-md-description">
          <p>Scale of total correlation loss within repeated samples. See :func:<code>losses.TotalCorrelationWithin</code>.</p>
        </div>
      </li>
      <li class="doc-section-item field-body">
        <b><code>delta</code></b>
            (<code>float</code>, default:
                <code>0.0</code>
)
        –
        <div class="doc-md-description">
          <p>Scale of similarity loss. See :func:<code>losses.Similarity</code>.</p>
        </div>
      </li>
      <li class="doc-section-item field-body">
        <b><code>delta_between</code></b>
            (<code>float</code>, default:
                <code>0.0</code>
)
        –
        <div class="doc-md-description">
          <p>Scale of similarity loss between repeated samples. See :func:<code>losses.SimilarityBetween</code>.</p>
        </div>
      </li>
      <li class="doc-section-item field-body">
        <b><code>kl_threshold</code></b>
            (<code>float</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Lower bound for the KL divergence. See :func:<code>losses.KLDivergence</code>.</p>
        </div>
      </li>
      <li class="doc-section-item field-body">
        <b><code>free_bits</code></b>
            (<code>float</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Number of bits to keep free for the KL divergence per latent dimension. See :func:<code>losses.KLDivergence</code>.</p>
        </div>
      </li>
      <li class="doc-section-item field-body">
        <b><code>repeat_samples</code></b>
            (<code>int</code>, default:
                <code>1</code>
)
        –
        <div class="doc-md-description">
          <p>Number of repetitions of input samples present in the batch.</p>
        </div>
      </li>
      <li class="doc-section-item field-body">
        <b><code>taper</code></b>
            (<code>array_like</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Numpy array of length <code>output_length</code> to taper the squared error. See :func:<code>losses.SquaredError</code>.</p>
        </div>
      </li>
  </ul>



  <p><span class="doc-section-title">Returns:</span></p>
  <ul>
      <li class="doc-section-item field-body">
            <code>callable</code>
        –
        <div class="doc-md-description">
          <p>Loss function that returns a tensor of shape <code>(batch_size, set_size, 1)</code>.</p>
      </div>
      </li>
  </ul>

          <details class="quote">
            <summary>Source code in <code>VAE/losses.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">VAEloss</span><span class="p">(</span><span class="n">z</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
            <span class="n">z_mean</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
            <span class="n">z_log_var</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
            <span class="n">beta</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">,</span>
            <span class="n">size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
            <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span>
            <span class="n">gamma_between</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span>
            <span class="n">gamma_within</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span>
            <span class="n">delta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span>
            <span class="n">delta_between</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span>
            <span class="n">kl_threshold</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">free_bits</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">repeat_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
            <span class="n">taper</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">callable</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Variational auto-encoder loss function.</span>

<span class="sd">    Loss function of variational auto-encoder, for use in :func:`models.VAE`. The input to the loss function has shape</span>
<span class="sd">    `(batch_size, set_size, output_length, channels)`. The output of the loss function has shape `(batch_size, set_size,</span>
<span class="sd">    1)`. The sample weights from the generator must have shape `(batch_size, set_size)`. This will make the sample</span>
<span class="sd">    weights sample-dependent; see also `sample_weight_mode=&#39;temporal&#39;` in model compile.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        z:</span>
<span class="sd">            Sample from latent space of shape `(batch_size, set_size, latent_dim)`.</span>
<span class="sd">        z_mean:</span>
<span class="sd">            Mean of latent space of shape `(batch_size, latent_dim)`.</span>
<span class="sd">        z_log_var:</span>
<span class="sd">            Log of variance of latent space of shape `(batch_size, latent_dim)`.</span>
<span class="sd">        size:</span>
<span class="sd">            Size of decoder output, i.e. total number of elements.</span>
<span class="sd">        beta:</span>
<span class="sd">            Loss weight of the KL divergence. If `beta` is a float, the loss weight is constant. If `beta` is a</span>
<span class="sd">            tensor, it should have shape `(batch_size, 1)`.</span>
<span class="sd">        gamma:</span>
<span class="sd">            Scale of total correlation loss. See :func:`losses.TotalCorrelation`.</span>
<span class="sd">        gamma_between:</span>
<span class="sd">            Scale of total correlation loss between repeated samples. See :func:`losses.TotalCorrelationBetween`.</span>
<span class="sd">        gamma_within:</span>
<span class="sd">            Scale of total correlation loss within repeated samples. See :func:`losses.TotalCorrelationWithin`.</span>
<span class="sd">        delta:</span>
<span class="sd">            Scale of similarity loss. See :func:`losses.Similarity`.</span>
<span class="sd">        delta_between:</span>
<span class="sd">            Scale of similarity loss between repeated samples. See :func:`losses.SimilarityBetween`.</span>
<span class="sd">        kl_threshold:</span>
<span class="sd">            Lower bound for the KL divergence. See :func:`losses.KLDivergence`.</span>
<span class="sd">        free_bits:</span>
<span class="sd">            Number of bits to keep free for the KL divergence per latent dimension. See :func:`losses.KLDivergence`.</span>
<span class="sd">        repeat_samples:</span>
<span class="sd">            Number of repetitions of input samples present in the batch.</span>
<span class="sd">        taper (array_like):</span>
<span class="sd">            Numpy array of length `output_length` to taper the squared error. See :func:`losses.SquaredError`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Loss function that returns a tensor of shape `(batch_size, set_size, 1)`.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="c1"># add singleton dimension to beta to shape (batch_size, 1, 1)</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">vae_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
        <span class="n">squared_error_fcn</span> <span class="o">=</span> <span class="n">SquaredError</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">,</span> <span class="n">taper</span><span class="o">=</span><span class="n">taper</span><span class="p">)</span>
        <span class="n">squared_error</span> <span class="o">=</span> <span class="n">squared_error_fcn</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

        <span class="n">kl_loss_fcn</span> <span class="o">=</span> <span class="n">KLDivergence</span><span class="p">(</span><span class="n">z_mean</span><span class="p">,</span> <span class="n">z_log_var</span><span class="p">,</span> <span class="n">kl_threshold</span><span class="o">=</span><span class="n">kl_threshold</span><span class="p">,</span> <span class="n">free_bits</span><span class="o">=</span><span class="n">free_bits</span><span class="p">)</span>
        <span class="n">entropy</span> <span class="o">=</span> <span class="n">kl_loss_fcn</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">gamma</span><span class="p">:</span>
            <span class="n">tc_loss_fcn</span> <span class="o">=</span> <span class="n">TotalCorrelation</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">z_mean</span><span class="p">,</span> <span class="n">z_log_var</span><span class="p">)</span>
            <span class="n">entropy</span> <span class="o">+=</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">tc_loss_fcn</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">gamma_between</span><span class="p">:</span>
            <span class="n">tc_between_loss_fcn</span> <span class="o">=</span> <span class="n">TotalCorrelationBetween</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">z_mean</span><span class="p">,</span> <span class="n">z_log_var</span><span class="p">,</span> <span class="n">repeat_samples</span><span class="o">=</span><span class="n">repeat_samples</span><span class="p">)</span>
            <span class="n">entropy</span> <span class="o">+=</span> <span class="n">gamma_between</span> <span class="o">*</span> <span class="n">tc_between_loss_fcn</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">gamma_within</span><span class="p">:</span>
            <span class="n">tc_within_loss_fcn</span> <span class="o">=</span> <span class="n">TotalCorrelationWithin</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">z_mean</span><span class="p">,</span> <span class="n">z_log_var</span><span class="p">,</span> <span class="n">repeat_samples</span><span class="o">=</span><span class="n">repeat_samples</span><span class="p">)</span>
            <span class="n">entropy</span> <span class="o">+=</span> <span class="n">gamma_within</span> <span class="o">*</span> <span class="n">tc_within_loss_fcn</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">delta</span><span class="p">:</span>
            <span class="n">sim_loss_fcn</span> <span class="o">=</span> <span class="n">Similarity</span><span class="p">()</span>
            <span class="n">entropy</span> <span class="o">+=</span> <span class="n">delta</span> <span class="o">*</span> <span class="n">sim_loss_fcn</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">delta_between</span><span class="p">:</span>
            <span class="n">sim_between_loss_fcn</span> <span class="o">=</span> <span class="n">SimilarityBetween</span><span class="p">(</span><span class="n">repeat_samples</span><span class="o">=</span><span class="n">repeat_samples</span><span class="p">)</span>
            <span class="n">entropy</span> <span class="o">+=</span> <span class="n">delta</span> <span class="o">*</span> <span class="n">sim_between_loss_fcn</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">squared_error</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">entropy</span>

    <span class="k">return</span> <span class="n">vae_loss</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="VAE.losses.VAEploss" class="doc doc-heading">
          <span class="doc doc-object-name doc-function-name">VAE.losses.VAEploss</span>


</h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">VAEploss</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">delta</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">delta_between</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">repeat_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">taper</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>VAE prediction loss function.</p>



  <p><span class="doc-section-title">Parameters:</span></p>
  <ul>
      <li class="doc-section-item field-body">
        <b><code>beta</code></b>
            (<code><span title="typing.Union">Union</span>[float, <span title="tensorflow.Tensor">Tensor</span>]</code>, default:
                <code>1.0</code>
)
        –
        <div class="doc-md-description">
          <p>Loss weight of the KL divergence. If <code>beta</code> is a float, the loss weight is constant. If <code>beta</code> is a
tensor, it should have shape <code>(batch_size, 1)</code>.</p>
        </div>
      </li>
      <li class="doc-section-item field-body">
        <b><code>delta</code></b>
            (<code>float</code>, default:
                <code>0.0</code>
)
        –
        <div class="doc-md-description">
          <p>Scale of similarity loss. See :func:<code>losses.Similarity</code>.</p>
        </div>
      </li>
      <li class="doc-section-item field-body">
        <b><code>delta_between</code></b>
            (<code>float</code>, default:
                <code>0.0</code>
)
        –
        <div class="doc-md-description">
          <p>Scale of similarity loss between repeated samples. See :func:<code>losses.SimilarityBetween</code>.</p>
        </div>
      </li>
      <li class="doc-section-item field-body">
        <b><code>size</code></b>
            (<code>int</code>, default:
                <code>1</code>
)
        –
        <div class="doc-md-description">
          <p>Size of prediction output, i.e. total number of elements.</p>
        </div>
      </li>
      <li class="doc-section-item field-body">
        <b><code>repeat_samples</code></b>
            (<code>int</code>, default:
                <code>1</code>
)
        –
        <div class="doc-md-description">
          <p>Number of repetitions of input samples present in the batch.</p>
        </div>
      </li>
      <li class="doc-section-item field-body">
        <b><code>taper</code></b>
            (<code>array_like</code>, default:
                <code>None</code>
)
        –
        <div class="doc-md-description">
          <p>Array of length <code>output_length</code> to taper the squared error. See :func:<code>losses.SquaredError</code>.</p>
        </div>
      </li>
  </ul>



  <p><span class="doc-section-title">Returns:</span></p>
  <ul>
      <li class="doc-section-item field-body">
            <code>callable</code>
        –
        <div class="doc-md-description">
          <p>Loss function that returns a tensor of shape <code>(batch_size, set_size, 1)</code>.</p>
      </div>
      </li>
  </ul>

          <details class="quote">
            <summary>Source code in <code>VAE/losses.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">VAEploss</span><span class="p">(</span><span class="n">beta</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">,</span>
             <span class="n">delta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span>
             <span class="n">delta_between</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span>
             <span class="n">repeat_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
             <span class="n">size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
             <span class="n">taper</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">callable</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;VAE prediction loss function.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        beta:</span>
<span class="sd">            Loss weight of the KL divergence. If `beta` is a float, the loss weight is constant. If `beta` is a</span>
<span class="sd">            tensor, it should have shape `(batch_size, 1)`.</span>
<span class="sd">        delta:</span>
<span class="sd">            Scale of similarity loss. See :func:`losses.Similarity`.</span>
<span class="sd">        delta_between:</span>
<span class="sd">            Scale of similarity loss between repeated samples. See :func:`losses.SimilarityBetween`.</span>
<span class="sd">        size:</span>
<span class="sd">            Size of prediction output, i.e. total number of elements.</span>
<span class="sd">        repeat_samples:</span>
<span class="sd">            Number of repetitions of input samples present in the batch.</span>
<span class="sd">        taper (array_like):</span>
<span class="sd">            Array of length `output_length` to taper the squared error. See :func:`losses.SquaredError`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Loss function that returns a tensor of shape `(batch_size, set_size, 1)`.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="c1"># add singleton dimension to beta to shape (batch_size, 1, 1)</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">vaep_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
        <span class="n">squared_error_fcn</span> <span class="o">=</span> <span class="n">SquaredError</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">,</span> <span class="n">taper</span><span class="o">=</span><span class="n">taper</span><span class="p">)</span>
        <span class="n">squared_error</span> <span class="o">=</span> <span class="n">squared_error_fcn</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

        <span class="n">entropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">squared_error</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">delta</span><span class="p">:</span>
            <span class="n">sim_loss_fcn</span> <span class="o">=</span> <span class="n">Similarity</span><span class="p">()</span>
            <span class="n">entropy</span> <span class="o">+=</span> <span class="n">delta</span> <span class="o">*</span> <span class="n">sim_loss_fcn</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">delta_between</span><span class="p">:</span>
            <span class="n">sim_between_loss_fcn</span> <span class="o">=</span> <span class="n">SimilarityBetween</span><span class="p">(</span><span class="n">repeat_samples</span><span class="o">=</span><span class="n">repeat_samples</span><span class="p">)</span>
            <span class="n">entropy</span> <span class="o">+=</span> <span class="n">delta_between</span> <span class="o">*</span> <span class="n">sim_between_loss_fcn</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">squared_error</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">entropy</span>

    <span class="k">return</span> <span class="n">vaep_loss</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="VAE.losses.example_total_correlation_losses" class="doc doc-heading">
          <span class="doc doc-object-name doc-function-name">VAE.losses.example_total_correlation_losses</span>


</h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">example_total_correlation_losses</span><span class="p">()</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Example of total correlation loss functions.</p>

          <details class="quote">
            <summary>Source code in <code>VAE/losses.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">example_total_correlation_losses</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Example of total correlation loss functions.&quot;&quot;&quot;</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
    <span class="n">repeat_samples</span> <span class="o">=</span> <span class="mi">20</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">repeat_samples</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
    <span class="n">set_size</span> <span class="o">=</span> <span class="mi">7</span>

    <span class="n">z_mean</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">z_log_var</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span> <span class="o">-</span> <span class="mf">1.</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">z_mean</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z_log_var</span> <span class="o">*</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">repeats</span><span class="o">=</span><span class="n">set_size</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">fcns</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;TC loss&#39;</span><span class="p">:</span> <span class="n">TotalCorrelation</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">z_mean</span><span class="p">,</span> <span class="n">z_log_var</span><span class="p">),</span>
        <span class="s1">&#39;TC loss between&#39;</span><span class="p">:</span> <span class="n">TotalCorrelationBetween</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">z_mean</span><span class="p">,</span> <span class="n">z_log_var</span><span class="p">,</span> <span class="n">repeat_samples</span><span class="o">=</span><span class="n">repeat_samples</span><span class="p">),</span>
        <span class="s1">&#39;TC loss within&#39;</span><span class="p">:</span> <span class="n">TotalCorrelationWithin</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">z_mean</span><span class="p">,</span> <span class="n">z_log_var</span><span class="p">,</span> <span class="n">repeat_samples</span><span class="o">=</span><span class="n">repeat_samples</span><span class="p">),</span>
    <span class="p">}</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="s2">&quot;Batch size&quot;</span><span class="si">:</span><span class="s1">&lt;20</span><span class="si">}</span><span class="s1"> </span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s1"> * </span><span class="si">{</span><span class="n">repeat_samples</span><span class="si">}</span><span class="s1"> = </span><span class="si">{</span><span class="n">batch_size</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">repeat_samples</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">fcn</span> <span class="ow">in</span> <span class="n">fcns</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">tc_loss</span> <span class="o">=</span> <span class="n">fcn</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">tc_mean</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tc_loss</span><span class="p">)</span>
        <span class="n">tc_std</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_std</span><span class="p">(</span><span class="n">tc_loss</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">name</span><span class="si">:</span><span class="s1">&lt;20</span><span class="si">}</span><span class="s1"> mean=</span><span class="si">{</span><span class="n">tc_mean</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">  std=</span><span class="si">{</span><span class="n">tc_std</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">  shape=</span><span class="si">{</span><span class="n">tc_loss</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="VAE.losses.example_similarity_losses" class="doc doc-heading">
          <span class="doc doc-object-name doc-function-name">VAE.losses.example_similarity_losses</span>


</h3>
<div class="doc-signature highlight"><pre><span></span><code><span class="nf">example_similarity_losses</span><span class="p">()</span>
</code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Example of similarity loss functions.</p>

          <details class="quote">
            <summary>Source code in <code>VAE/losses.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">example_similarity_losses</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Example of similarity loss functions.&quot;&quot;&quot;</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
    <span class="n">repeat_samples</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">repeat_samples</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">160</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>

    <span class="n">fcns</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;Sim loss&#39;</span><span class="p">:</span> <span class="n">Similarity</span><span class="p">(),</span>
        <span class="s1">&#39;Sim loss between&#39;</span><span class="p">:</span> <span class="n">_SimilarityBetween</span><span class="p">(</span><span class="n">repeat_samples</span><span class="o">=</span><span class="n">repeat_samples</span><span class="p">),</span>
        <span class="s1">&#39;Sim loss between (fast)&#39;</span><span class="p">:</span> <span class="n">SimilarityBetween</span><span class="p">(</span><span class="n">repeat_samples</span><span class="o">=</span><span class="n">repeat_samples</span><span class="p">),</span>
    <span class="p">}</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="s2">&quot;Batch size&quot;</span><span class="si">:</span><span class="s1">&lt;25</span><span class="si">}</span><span class="s1"> </span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s1"> * </span><span class="si">{</span><span class="n">repeat_samples</span><span class="si">}</span><span class="s1"> = </span><span class="si">{</span><span class="n">batch_size</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">repeat_samples</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">fcn</span> <span class="ow">in</span> <span class="n">fcns</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">fcn</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
        <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="n">mean_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="n">std_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_std</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">name</span><span class="si">:</span><span class="s1">&lt;25</span><span class="si">}</span><span class="s1"> mean=</span><span class="si">{</span><span class="n">mean_loss</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">  std=</span><span class="si">{</span><span class="n">std_loss</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">  shape=</span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">losses</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2023 Andreas Groth
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/andr-groth" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.top", "navigation.sections", "navigation.expand", "content.code.copy", "search.suggest", "search.highlight"], "search": "../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.a7c05c9e.min.js"></script>
      
    
  </body>
</html>